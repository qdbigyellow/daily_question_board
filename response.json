{
  "timestamp": "2026-02-20 09:39:34 UTC",
  "response": "Here are three multiple-choice questions designed to meet your specifications:\n\n---\n\n### Question 1: AWS Architect (Intermediate Level)\n\n**The Question:**\nA rapidly growing e-commerce company needs to migrate its existing monolithic web application from on-premises to AWS. The application experiences highly unpredictable traffic patterns, including seasonal peaks that require significant scaling. It processes sensitive customer data and needs to meet strict RTO/RPO targets of under 15 minutes in case of an Availability Zone (AZ) outage. Cost optimization for variable loads is also a major concern. Which AWS architectural pattern best addresses these requirements?\n\nA. Deploy the monolithic application on a single, large Amazon EC2 instance within a single Availability Zone, backed by Amazon EBS for storage. Use scheduled EC2 instance scaling during peak seasons.\nB. Break down the monolith into microservices. Each microservice should run on AWS Lambda functions, leveraging Amazon API Gateway for routing and Amazon DynamoDB for all data persistence.\nC. Deploy the monolithic application across an Auto Scaling Group of Amazon EC2 instances within multiple Availability Zones, behind an Application Load Balancer (ALB). Utilize Amazon RDS in Multi-AZ configuration for the database and store static assets in Amazon S3.\nD. Containerize the application and deploy it on a single Amazon ECS cluster using EC2 launch type across multiple Availability Zones. Use Amazon Aurora Serverless for the database and Amazon SQS for inter-service communication.\n\n**Answer Options:**\nA. Deploy the monolithic application on a single, large Amazon EC2 instance within a single Availability Zone, backed by Amazon EBS for storage. Use scheduled EC2 instance scaling during peak seasons.\nB. Break down the monolith into microservices. Each microservice should run on AWS Lambda functions, leveraging Amazon API Gateway for routing and Amazon DynamoDB for all data persistence.\nC. Deploy the monolithic application across an Auto Scaling Group of Amazon EC2 instances within multiple Availability Zones, behind an Application Load Balancer (ALB). Utilize Amazon RDS in Multi-AZ configuration for the database and store static assets in Amazon S3.\nD. Containerize the application and deploy it on a single Amazon ECS cluster using EC2 launch type across multiple Availability Zones. Use Amazon Aurora Serverless for the database and Amazon SQS for inter-service communication.\n\n**Correct Answer Explanation:**\n\n*   **Correct Answer: C**\n    *   **Auto Scaling Group (ASG) with EC2 in Multiple AZs:** Directly addresses unpredictable traffic and high availability (HA) by distributing instances and automatically scaling them up/down based on demand, leading to cost optimization for variable loads. Multiple AZs ensure fault tolerance.\n    *   **Application Load Balancer (ALB):** Distributes incoming web traffic across instances in the ASG, supporting advanced routing features suitable for web applications and ensuring HA.\n    *   **Amazon RDS Multi-AZ:** Provides high availability and fault tolerance for the database, with automatic failover to a standby instance in a different AZ, crucial for RTO/RPO targets for sensitive data.\n    *   **Amazon S3 for Static Assets:** Cost-effective, highly available, and scalable storage for static content, offloading the EC2 instances.\n    *   This architecture directly supports a \"monolithic\" application migration without requiring a full re-architecture to microservices (which can be a separate, complex project), while meeting the HA, fault tolerance, scalability, and cost optimization requirements for unpredictable loads and strict RTO/RPO.\n\n**Why Other Answers Are Wrong:**\n\n*   **A. (Single EC2, Single AZ, Scheduled Scaling):** This option fails to meet high availability (single AZ is a single point of failure), fault tolerance, and flexible scaling requirements. Scheduled scaling doesn't react to unpredictable spikes and a single instance creates a bottleneck.\n*   **B. (Lambda Microservices, DynamoDB):** While highly scalable and cost-effective, this option proposes a complete re-architecture from a \"monolithic\" application to microservices on serverless. This is a significant undertaking, not just an architectural *migration*, and the question implies a need to migrate the *existing* monolith. Additionally, migrating *all* data persistence to DynamoDB might not be suitable or cost-effective for all relational data structures typically found in a monolith without considerable schema redesign.\n*   **D. (ECS on EC2, Aurora Serverless, SQS for Inter-Service Communication):** While ECS with EC2 launch type across multiple AZs and Aurora Serverless (for cost efficiency with variable loads) are good choices for HA and scalability, the problem explicitly states a \"monolithic web application.\" Aurora Serverless might be suitable for a monolith database, but SQS for \"inter-service communication\" implies a microservices architecture, which contradicts the \"monolithic\" nature of the application as given. If the goal is to migrate the monolith, introducing microservice communication patterns doesn't fit the immediate problem description. This option also doesn't explicitly mention auto-scaling for the EC2 hosts within the ECS cluster, which is critical for variable loads.\n\n---\n\n### Question 2: Terraform Script (Intermediate Level)\n\n**The Question:**\nYou are developing a Terraform configuration to provision multiple AWS S3 buckets. Each bucket needs to be created with a unique name and a specific set of tags based on a variable map. After each bucket is successfully provisioned, a unique local shell script must be executed to perform post-creation setup tasks specific to that individual bucket (e.g., uploading initial configuration files, registering the bucket with an external service).\n\nWhich of the following Terraform configurations correctly implements this requirement, ensuring the local script runs *once per bucket* and *after* the bucket's creation?\n\nA.\n```terraform\nvariable \"bucket_configs\" {\n  type = map(object({\n    name = string\n    tags = map(string)\n  }))\n  default = {\n    \"app1_logs\" = { name = \"my-app1-logs\", tags = { Project = \"App1\" } }\n    \"app2_data\" = { name = \"my-app2-data\", tags = { Project = \"App2\" } }\n  }\n}\n\nresource \"aws_s3_bucket\" \"app_buckets\" {\n  count  = length(var.bucket_configs)\n  bucket = values(var.bucket_configs)[count.index].name\n  tags   = values(var.bucket_configs)[count.index].tags\n\n  provisioner \"local-exec\" {\n    command = \"echo 'Initializing bucket ${self.bucket}' && ./init_script.sh ${self.bucket} ${each.value.name}\"\n  }\n}\n```\nB.\n```terraform\nvariable \"bucket_configs\" {\n  type = map(object({\n    name = string\n    tags = map(string)\n  }))\n  default = {\n    \"app1_logs\" = { name = \"my-app1-logs\", tags = { Project = \"App1\" } }\n    \"app2_data\" = { name = \"my-app2-data\", tags = { Project = \"App2\" } }\n  }\n}\n\nresource \"aws_s3_bucket\" \"app_buckets\" {\n  for_each = var.bucket_configs\n  bucket   = each.value.name\n  tags     = each.value.tags\n}\n\nresource \"null_resource\" \"bucket_initializer\" {\n  for_each = aws_s3_bucket.app_buckets\n\n  triggers = {\n    bucket_id = each.value.id\n  }\n\n  provisioner \"local-exec\" {\n    command = \"echo 'Initializing bucket ${each.value.bucket}' && ./init_script.sh ${each.value.bucket}\"\n  }\n}\n```\nC.\n```terraform\nvariable \"bucket_configs\" {\n  type = map(object({\n    name = string\n    tags = map(string)\n  }))\n  default = {\n    \"app1_logs\" = { name = \"my-app1-logs\", tags = { Project = \"App1\" } }\n    \"app2_data\" = { name = \"my-app2-data\", tags = { Project = \"App2\" } }\n  }\n}\n\nresource \"aws_s3_bucket\" \"app_buckets\" {\n  for_each = var.bucket_configs\n  bucket   = each.value.name\n  tags     = each.value.tags\n}\n\nresource \"null_resource\" \"init_all_buckets\" {\n  depends_on = [aws_s3_bucket.app_buckets]\n\n  provisioner \"local-exec\" {\n    command = \"echo 'Initializing all buckets: ${join(\",\", [for b in aws_s3_bucket.app_buckets : b.bucket])}' && ./batch_init_script.sh ${join(\" \", [for b in aws_s3_bucket.app_buckets : b.bucket])}\"\n  }\n}\n```\nD.\n```terraform\nvariable \"bucket_configs\" {\n  type = map(object({\n    name = string\n    tags = map(string)\n  }))\n  default = {\n    \"app1_logs\" = { name = \"my-app1-logs\", tags = { Project = \"App1\" } }\n    \"app2_data\" = { name = \"my-app2-data\", tags = { Project = \"App2\" } }\n  }\n}\n\nresource \"aws_s3_bucket\" \"app_buckets\" {\n  for_each = var.bucket_configs\n  bucket   = each.value.name\n  tags     = each.value.tags\n\n  provisioner \"local-exec\" {\n    command = \"echo 'Initializing bucket ${self.bucket}' && ./init_script.sh ${self.bucket}\"\n  }\n}\n```\n\n**Correct Answer Explanation:**\n\n*   **Correct Answer: B**\n    *   **`for_each` on `aws_s3_bucket`:** Correctly iterates over the `bucket_configs` map to create multiple S3 buckets, each with unique properties (`each.value.name`, `each.value.tags`).\n    *   **`null_resource` with `for_each`:** This is the key. By applying `for_each = aws_s3_bucket.app_buckets` to a `null_resource`, a separate `null_resource` instance is created for each S3 bucket. Terraform implicitly understands the dependency: each `null_resource` instance will only run its `provisioner` after its corresponding `aws_s3_bucket` instance has been successfully created.\n    *   **`triggers` argument in `null_resource`:** Although the `for_each` dependency often suffices for ordering, adding `triggers = { bucket_id = each.value.id }` ensures that if the underlying S3 bucket's ID (or any other attribute used in the trigger) changes, the `null_resource` and its `local-exec` provisioner will be re-evaluated.\n    *   **`each.value.bucket` in `local-exec`:** This correctly accesses the `bucket` attribute (the name) of the specific S3 bucket resource instance being processed by the current `null_resource` iteration, ensuring the script is specific to that bucket.\n\n**Why Other Answers Are Wrong:**\n\n*   **A. (Using `count` with inline `provisioner`):**\n    *   Using `count` is generally less preferred than `for_each` when iterating over maps, as `for_each` provides clearer resource addressing.\n    *   More critically, `self.bucket` is deprecated and often unreliable within `provisioner` blocks for resources created with `count`. You would typically need to reference `aws_s3_bucket.app_buckets[count.index].bucket` to get the correct bucket name, making `self.bucket` incorrect here.\n    *   The `each.value.name` is invalid when `count` is used as `each` is not available directly in this context (you'd use `values(var.bucket_configs)[count.index].name`).\n*   **C. (Single `null_resource` for all buckets):** This option creates only *one* `null_resource` that runs a *single* `local-exec` provisioner. The requirement was for a script to be executed \"once per bucket\" and \"specific to that individual bucket.\" This configuration would attempt to initialize all buckets in one go, which doesn't meet the \"specific to that individual bucket\" and \"once per bucket\" independent execution criteria.\n*   **D. (Inline `provisioner` with `for_each`):** While `for_each` is used correctly for `aws_s3_bucket`, using an inline `provisioner` directly within a resource block that uses `for_each` can be problematic. Inline provisioners are generally less robust for post-creation actions in `for_each` scenarios compared to a dedicated `null_resource`. More importantly, `self.bucket` is not consistently reliable when used within a resource created with `for_each` (you would typically use `each.value.name` or `aws_s3_bucket.app_buckets[each.key].bucket` for explicit reference, although `self` sometimes works, it's not the idiomatic way for `for_each`). The `null_resource` approach (Option B) provides a cleaner separation and more reliable dependency management for post-creation tasks for each individual item.\n\n---\n\n### Question 3: Python Programming (Advanced Level)\n\n**The Question:**\nYou are tasked with implementing a memoization decorator in Python for a computationally intensive function. This decorator should cache the results of function calls based on their arguments. The function being decorated can accept both positional and keyword arguments, and all arguments (including values within keyword arguments) are guaranteed to be hashable. The cache should be specific to each decorated function instance.\n\nWhich of the following implementations correctly defines such a memoization decorator?\n\nA.\n```python\ndef memoize(func):\n    cache = {}\n    def wrapper(*args, **kwargs):\n        if args in cache:\n            return cache[args]\n        result = func(*args, **kwargs)\n        cache[args] = result\n        return result\n    return wrapper\n```\n\nB.\n```python\ndef memoize(func):\n    cache = {}\n    def wrapper(*args, **kwargs):\n        # Create a hashable key from args and kwargs\n        key = (args, frozenset(kwargs.items()))\n        if key in cache:\n            return cache[key]\n        result = func(*args, **kwargs)\n        cache[key] = result\n        return result\n    return wrapper\n```\n\nC.\n```python\ndef memoize(func):\n    def wrapper(*args, **kwargs):\n        if not hasattr(wrapper, '_cache'): # Cache is attached to wrapper, not func\n            wrapper._cache = {}\n        key = (args, tuple(sorted(kwargs.items())))\n        if key in wrapper._cache:\n            return wrapper._cache[key]\n        result = func(*args, **kwargs)\n        wrapper._cache[key] = result\n        return result\n    return wrapper\n```\n\nD.\n```python\nclass Memoize:\n    def __init__(self, func):\n        self.func = func\n        self.cache = {}\n\n    def __call__(self, *args, **kwargs):\n        # Unreliable key generation using string representation\n        key = hash(str(args) + str(kwargs)) \n        if key in self.cache:\n            return self.cache[key]\n        result = self.func(*args, **kwargs)\n        self.cache[key] = result\n        return result\n\ndef memoize(func):\n    return Memoize(func)\n```\n\n**Correct Answer Explanation:**\n\n*   **Correct Answer: B**\n    *   **Closure for `cache`:** The `cache` dictionary is defined in the `memoize` function's scope, making it a closure variable for the `wrapper` function. This ensures that each time `@memoize` is used to decorate a *different* function, a *new* `cache` dictionary is created, specific to that decorated function instance.\n    *   **Hashable Key for `args` and `kwargs`:** The key `(args, frozenset(kwargs.items()))` correctly handles both positional and keyword arguments.\n        *   `args` is already a tuple (and thus hashable).\n        *   `kwargs.items()` returns a view of key-value pairs. Using `frozenset()` on these items creates an immutable, hashable set of (key, value) tuples. This is crucial because `dict.items()` itself is not hashable, and `frozenset` correctly handles the arbitrary order of keyword arguments in the `kwargs` dictionary, ensuring that `f(a=1, b=2)` and `f(b=2, a=1)` produce the same cache key.\n        *   The combination of these into a final tuple `(args, frozenset(...))` forms a single, hashable, and canonical key for the cache.\n\n**Why Other Answers Are Wrong:**\n\n*   **A. (Ignores `kwargs` in key):** The cache key is simply `args`. This means if a function is called with `my_func(1, a=10)` and then `my_func(1, b=20)`, the cache will incorrectly return the result of the first call because it only checked the positional argument `(1,)`. It fails to consider keyword arguments for memoization.\n*   **C. (Problematic key generation and cache attachment):**\n    *   **Cache attachment:** Attaching `_cache` to `wrapper` itself (`wrapper._cache = {}`) is a valid way to create a cache specific to the *returned wrapper function*, which in effect is specific to the decorated function instance. It works similarly to the closure approach in B.\n    *   **Key generation:** The key `(args, tuple(sorted(kwargs.items())))` is problematic. While `sorted(kwargs.items())` ensures a consistent order for keyword arguments, if any value within `kwargs` is uncomparable (e.g., custom objects without `__lt__`), `sorted()` will raise a `TypeError`. `frozenset` (as in option B) is more robust for creating a hashable key from dictionary items when values are guaranteed to be hashable but not necessarily comparable.\n*   **D. (Unreliable key generation):**\n    *   **Class-based decorator:** Using a class with `__call__` for a decorator is a valid and often more powerful pattern.\n    *   **Key generation:** The key `hash(str(args) + str(kwargs))` is fundamentally unreliable and inefficient.\n        *   `str(kwargs)` might produce different string representations for the same logical keyword arguments (due to dictionary item order variations) across different Python runs or versions, leading to cache misses.\n        *   Hashing a string representation is generally less efficient and more collision-prone than directly hashing immutable data structures like tuples and `frozenset`.\n        *   There's no guarantee that `str()` produces a unique string for unique argument combinations, although for simple types it often does."
}

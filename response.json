{
  "timestamp": "2025-12-04 09:23:40 UTC",
  "response": "Here are three multiple-choice questions designed to meet your specifications:\n\n---\n\n### **1. AWS Architect (Intermediate Level)**\n\n**Question:**\nA company is deploying a critical e-commerce application on AWS. The application requires a highly available, fault-tolerant relational database backend capable of sustaining high transaction rates. In addition, the solution must provide a robust disaster recovery strategy with a low RTO (Recovery Time Objective) and RPO (Recovery Point Objective) across multiple AWS regions. Which AWS database architecture best meets these requirements?\n\n**Answer Options:**\nA. Deploy an Amazon RDS PostgreSQL instance in a Multi-AZ configuration with automated backups enabled.\nB. Run a self-managed PostgreSQL database on Amazon EC2 instances configured with a primary and a standby in different Availability Zones, using EBS snapshots for cross-region backup.\nC. Implement Amazon Aurora PostgreSQL with a Multi-AZ cluster in the primary region and an Aurora Global Database spanning a secondary region.\nD. Use Amazon DynamoDB in On-Demand capacity mode with global tables enabled.\n\n**Correct Answer:** C\n\n**Explanation:**\n*   **A. Incorrect:** Amazon RDS Multi-AZ provides high availability and fault tolerance within a single region, but its cross-region disaster recovery (e.g., using manual snapshots or read replicas) typically has higher RTO/RPO compared to a dedicated multi-region solution like Aurora Global Database.\n*   **B. Incorrect:** While technically possible, a self-managed database on EC2 instances incurs significant operational overhead for high availability, patching, backups, and especially for low RTO/RPO cross-region disaster recovery. EBS snapshots for cross-region backup generally do not offer the low RPO/RTO required for critical applications compared to managed services.\n*   **C. Correct:** Amazon Aurora PostgreSQL provides superior performance and scalability compared to standard RDS, and its Multi-AZ cluster ensures high availability and fault tolerance within the primary region. The key for low RTO/RPO cross-region disaster recovery is **Aurora Global Database**. It uses dedicated infrastructure for asynchronous replication with typical RPO in seconds and RTO of less than a minute for regional outages, specifically designed for mission-critical applications needing robust multi-region DR.\n*   **D. Incorrect:** Amazon DynamoDB is a NoSQL database, and the requirement explicitly states a \"relational database backend.\" While DynamoDB Global Tables offer multi-region replication and low latency, it does not meet the relational requirement.\n\n---\n\n### **2. Terraform Script (Intermediate Level)**\n\n**Question:**\nYou need to provision several AWS S3 buckets, each with a unique name and potentially different configurations (e.g., tags, versioning status). You want to define these bucket properties in a single input variable and use Terraform's `for_each` meta-argument to create them efficiently.\n\nGiven the following `variables.tf`:\n```terraform\nvariable \"bucket_configs\" {\n  description = \"Map of S3 bucket configurations.\"\n  type = map(object({\n    name              = string\n    versioning_enabled = bool\n    tags              = map(string)\n  }))\n  default = {\n    \"app-logs\" = {\n      name              = \"my-app-logs-bucket\"\n      versioning_enabled = true\n      tags              = {\n        Environment = \"dev\"\n        Purpose     = \"Logs\"\n      }\n    },\n    \"static-assets\" = {\n      name              = \"my-static-assets-bucket\"\n      versioning_enabled = false\n      tags              = {\n        Environment = \"dev\"\n        Purpose     = \"Web Assets\"\n      }\n    }\n  }\n}\n```\nWhich `main.tf` configuration correctly provisions these S3 buckets using `for_each`?\n\n**Answer Options:**\nA.\n```terraform\nresource \"aws_s3_bucket\" \"example\" {\n  for_each = var.bucket_configs\n  bucket   = each.value.name\n  versioning {\n    enabled = each.value.versioning_enabled\n  }\n  tags = each.value.tags\n}\n```\nB.\n```terraform\nresource \"aws_s3_bucket\" \"example\" {\n  for_each = var.bucket_configs\n  bucket   = each.key.name\n  versioning {\n    enabled = each.key.versioning_enabled\n  }\n  tags = each.key.tags\n}\n```\nC.\n```terraform\nresource \"aws_s3_bucket\" \"example\" {\n  for_each = var.bucket_configs[*]\n  bucket   = each.value.name\n  versioning {\n    enabled = each.value.versioning_enabled\n  }\n  tags = each.value.tags\n}\n```\nD.\n```terraform\nresource \"aws_s3_bucket\" \"example\" {\n  count = length(var.bucket_configs)\n  bucket = var.bucket_configs[count.index].name\n  versioning {\n    enabled = var.bucket_configs[count.index].versioning_enabled\n  }\n  tags = var.bucket_configs[count.index].tags\n}\n```\n\n**Correct Answer:** A\n\n**Explanation:**\n*   **A. Correct:** This is the correct and idiomatic way to use `for_each` with a map variable where the map values are objects. `for_each` iterates over the keys of `var.bucket_configs`. Inside the loop, `each.key` refers to the map key (e.g., \"app-logs\", \"static-assets\"), and `each.value` refers to the corresponding object (e.g., `{name=\"my-app-logs-bucket\", ...}`). We then correctly access the properties of `each.value` like `each.value.name`, `each.value.versioning_enabled`, and `each.value.tags`.\n*   **B. Incorrect:** `each.key` would be the string \"app-logs\" or \"static-assets\", not the object containing the bucket configuration. Attempting to access `each.key.name` would result in an error because a string does not have a `name` attribute.\n*   **C. Incorrect:** `var.bucket_configs[*]` is a splat expression that would produce a list of the objects (the values) from the map. While `for_each` can iterate over a set of strings, passing a list of objects directly to `for_each` as `var.bucket_configs[*]` is not the correct or idiomatic usage. `for_each` expects a map or a set. If it receives a list, Terraform will attempt to convert it to a set, which is unpredictable if the objects are not unique or hashable in a way Terraform expects for set elements. The cleanest way to iterate over a map is to pass the map directly.\n*   **D. Incorrect:** `count` is used for creating multiple instances when the input is typically a list, and resources are accessed via `count.index`. The `var.bucket_configs` variable is defined as a `map`, not a list. Attempting to access `var.bucket_configs[count.index]` on a map would result in an error, as maps are indexed by keys (strings) not integer indices.\n\n---\n\n### **3. Python Programming (Advanced Level)**\n\n**Question:**\nYou are tasked with processing a potentially very large sequence of numbers. For memory efficiency, you need to square each number lazily, meaning that the squaring operation only occurs when the next squared number is explicitly requested, rather than pre-calculating and storing all results.\n\nWhich of the following Python implementations correctly and most efficiently achieves this lazy squaring behavior for an arbitrary iterable of numbers, adhering to advanced Python principles?\n\n**Answer Options:**\nA.\n```python\ndef lazy_square(numbers):\n    squared_list = []\n    for num in numbers:\n        squared_list.append(num * num)\n    return squared_list\n```\nB.\n```python\nclass LazySquareIterator:\n    def __init__(self, numbers):\n        self._iterator = iter(numbers)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        try:\n            num = next(self._iterator)\n            return num * num\n        except StopIteration:\n            raise\n```\nC.\n```python\ndef lazy_square(numbers):\n    return map(lambda x: x * x, numbers)\n```\nD.\n```python\ndef lazy_square(numbers):\n    for num in numbers:\n        yield num * num\n```\n\n**Correct Answer:** D\n\n**Explanation:**\n*   **A. Incorrect:** This implementation eagerly processes all numbers and stores all squared results in a list (`squared_list`) before returning. This is memory-inefficient for large sequences, as it requires storing all results in memory simultaneously. It does not achieve lazy evaluation.\n*   **B. Correct, but not the *most* Pythonic/efficient for this simple case:** This correctly implements the iterator protocol (`__iter__` and `__next__`) to create a custom lazy iterator. It's memory-efficient because it calculates and yields one squared number at a time. While technically correct and demonstrating advanced understanding of the iterator protocol, for this specific transformation, there's a more concise and idiomatic Pythonic way.\n*   **C. Correct, but less direct demonstration of *advanced* principles than D:** `map()` is a built-in function that returns an iterator, making it lazy and memory-efficient. It's concise and performs well. However, the question asks to \"adher[e] to advanced Python principles\" and implicitly to *demonstrate* deep understanding. While `map` *uses* iteration, a generator function (`yield`) directly *creates* an iterator using a core advanced language feature, offering more control and flexibility for complex lazy sequences.\n*   **D. Correct and Most Advanced/Pythonic:** This uses a generator function, which is the most idiomatic and concise way in Python to create a lazy sequence for custom transformations. The `yield` keyword makes the function a generator, which returns an iterator. Each `yield` pauses the function's execution and returns a value, resuming from where it left off on the next call to `next()`. This achieves lazy evaluation and is highly memory-efficient, processing one item at a time without storing intermediate results. This directly demonstrates an advanced understanding of Python's generator mechanism."
}

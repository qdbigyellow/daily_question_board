{
  "timestamp": "2026-01-05 09:31:05 UTC",
  "response": "Here are three multiple-choice questions, each meeting your specified criteria:\n\n---\n\n### Question 1: AWS Architect (Intermediate Level)\n\n**Question:** A company runs a critical, stateful web application in AWS with a MySQL database. The application needs to be highly available and resilient to an entire AWS Region outage, aiming for a Recovery Time Objective (RTO) of less than 4 hours and a Recovery Point Objective (RPO) of less than 1 hour. The current setup is deployed in a single region across multiple Availability Zones. Which architecture strategy would best meet these disaster recovery requirements?\n\n**Answer Options:**\n\nA) Implement an active-passive multi-region setup: Replicate the application stack in a secondary region, use Amazon RDS Cross-Region Read Replicas for the database, and configure Amazon Route 53 with failover routing policies to redirect traffic in case of a primary region outage.\nB) Deploy the application across multiple Availability Zones within the same region and use AWS Backup for daily snapshots of the database, replicated to a different region.\nC) Migrate the database to Amazon DynamoDB Global Tables and distribute the application instances across multiple EC2 Regions directly under a single Application Load Balancer (ALB).\nD) Configure S3 Cross-Region Replication for the application's static content and deploy a read-only replica of the database in a secondary region using custom scripts for data synchronization.\n\n**Correct Answer:** A\n\n**Explanation:**\n*   **A) Correct.** This option outlines a standard and robust active-passive multi-region disaster recovery strategy. RDS Cross-Region Read Replicas provide low RPO by continuously replicating data, making them suitable for an RPO of less than 1 hour. Route 53 failover routing policies enable rapid traffic redirection (contributing to an RTO of less than 4 hours) to a pre-deployed, ready-to-activate application stack in the secondary region.\n*   **B) Incorrect.** Deploying across multiple Availability Zones within the same region protects against AZ failures, but not an entire region outage. While AWS Backup can replicate snapshots to another region, restoring an entire database and application stack from backups would likely exceed an RTO of 4 hours for a critical application.\n*   **C) Incorrect.** The question specifies a MySQL database, making DynamoDB Global Tables (a NoSQL service) an inappropriate solution for the current database. Furthermore, a single Application Load Balancer (ALB) cannot span multiple AWS Regions; it operates within a single region and distributes traffic across AZs.\n*   **D) Incorrect.** S3 Cross-Region Replication is for object storage (like static content), not for a relational database. While a read-only replica in a secondary region is a step, relying on \"custom scripts for data synchronization\" for the primary database replication introduces complexity, potential for errors, and makes guaranteeing RPO/RTO significantly harder compared to managed solutions like RDS Cross-Region Read Replicas.\n\n---\n\n### Question 2: Terraform Script (Intermediate Level)\n\n**Question:** An operations team needs to deploy three AWS S3 buckets with the following specific names: `app-logs-prod-us-east-1`, `backup-data-prod-us-east-1`, and `user-uploads-prod-us-east-1`. All buckets must have versioning enabled and default server-side encryption with AES256. The team wants to achieve this efficiently using a single `aws_s3_bucket` resource block in Terraform.\n\nWhich of the following Terraform configurations correctly achieves this requirement?\n\n**Answer Options:**\n\nA)\n```terraform\nresource \"aws_s3_bucket\" \"my_buckets\" {\n  count = 3\n  bucket = element([\"app-logs-prod-us-east-1\", \"backup-data-prod-us-east-1\", \"user-uploads-prod-us-east-1\"], count.index)\n\n  versioning {\n    enabled = true\n  }\n\n  server_side_encryption_configuration {\n    rule {\n      apply_server_side_encryption_by_default {\n        sse_algorithm = \"AES256\"\n      }\n    }\n  }\n}\n```\n\nB)\n```terraform\nresource \"aws_s3_bucket\" \"my_buckets\" {\n  for_each = toset([\n    \"app-logs-prod-us-east-1\",\n    \"backup-data-prod-us-east-1\",\n    \"user-uploads-prod-us-east-1\"\n  ])\n  bucket = each.key\n\n  versioning {\n    enabled = true\n  }\n\n  server_side_encryption_configuration {\n    rule {\n      apply_server_side_encryption_by_default {\n        sse_algorithm = \"AES256\"\n      }\n    }\n  }\n}\n```\n\nC)\n```terraform\nvariable \"bucket_names\" {\n  type = list(string)\n  default = [\n    \"app-logs-prod-us-east-1\",\n    \"backup-data-prod-us-east-1\",\n    \"user-uploads-prod-us-east-1\"\n  ]\n}\n\nresource \"aws_s3_bucket\" \"my_buckets\" {\n  bucket = var.bucket_names\n  # Versioning and encryption configuration here...\n}\n```\n\nD)\n```terraform\nresource \"aws_s3_bucket\" \"my_buckets\" {\n  name_prefix = \"my-s3-bucket-\"\n  count = 3\n\n  versioning {\n    enabled = true\n  }\n\n  server_side_encryption_configuration {\n    rule {\n      apply_server_side_encryption_by_default {\n        sse_algorithm = \"AES256\"\n      }\n    }\n  }\n}\n```\n\n**Correct Answer:** B\n\n**Explanation:**\n*   **B) Correct.** This configuration uses the `for_each` meta-argument, which is the most idiomatic and robust way to create multiple instances of a resource when each instance has a unique, identifiable key (like a bucket name). `toset` converts the list of names into a set, ensuring unique keys, and `each.key` correctly assigns each name to the `bucket` attribute. This approach provides excellent state management if individual buckets need to be added, removed, or modified later.\n*   **A) Partially Correct, but less idiomatic.** This configuration uses the `count` meta-argument, which is also a valid way to create multiple resources. `element(list, count.index)` correctly retrieves each bucket name from the list. However, `for_each` is generally preferred over `count` when unique names are involved, as `for_each` maps each resource instance to a distinct key, improving clarity and state stability when items are added, removed, or reordered in the input collection. While functional, it's not the *most* idiomatic or robust choice for this specific scenario compared to `for_each`.\n*   **C) Incorrect.** The `bucket` attribute of the `aws_s3_bucket` resource expects a single string value representing the bucket name, not a list of strings. Assigning a list directly would result in a Terraform error.\n*   **D) Incorrect.** The `aws_s3_bucket` resource does not support a `name_prefix` attribute for creating multiple buckets from a single resource block. The `name_prefix` attribute is used for *some* other AWS resources (e.g., `aws_instance`) to generate a unique name by appending random characters to a given prefix for a *single* resource.\n\n---\n\n### Question 3: Python Programming (Advanced Level)\n\n**Question:** You are tasked with processing a potentially *very large* stream of integer data from a source (represented as an iterable). The processing involves two stages:\n1.  **Filtering:** Keep only numbers that are divisible by 3.\n2.  **Transformation:** For each filtered number, calculate its square.\n\nThe final output should be an iterable of these squared numbers. Due to the potentially massive size of the input stream, the solution must be memory-efficient, processing numbers one by one without storing intermediate lists in their entirety.\n\nWhich of the following Python implementations correctly and most efficiently achieves this using only built-in features?\n\n**Answer Options:**\n\nA)\n```python\ndef process_stream_A(data_stream):\n    filtered_numbers = []\n    for x in data_stream:\n        if x % 3 == 0:\n            filtered_numbers.append(x)\n    \n    squared_numbers = []\n    for x in filtered_numbers:\n        squared_numbers.append(x * x)\n    return squared_numbers\n```\n\nB)\n```python\ndef process_stream_B(data_stream):\n    def filter_divisible_by_3(stream):\n        for x in stream:\n            if x % 3 == 0:\n                yield x\n\n    def square_numbers(stream):\n        for x in stream:\n            yield x * x\n    \n    return square_numbers(filter_divisible_by_3(data_stream))\n```\n\nC)\n```python\ndef process_stream_C(data_stream):\n    return [x * x for x in data_stream if x % 3 == 0]\n```\n\nD)\n```python\ndef process_stream_D(data_stream):\n    global _temp_results\n    _temp_results = []\n    for x in data_stream:\n        if x % 3 == 0:\n            _temp_results.append(x * x)\n    return iter(_temp_results)\n```\n\n**Correct Answer:** B\n\n**Explanation:**\n*   **B) Correct.** This solution uses nested generator functions (`yield`) to create a processing pipeline. `filter_divisible_by_3` is a generator that yields numbers divisible by 3 one by one. This generator's output then serves as the input `stream` for `square_numbers`, which is another generator that yields the square of each received number, again one by one. At no point are large intermediate lists created in memory, making it highly memory-efficient for very large data streams.\n*   **A) Incorrect.** This implementation creates two full intermediate lists (`filtered_numbers` and `squared_numbers`) in memory. For a \"very large\" data stream, this would consume a significant amount of memory, potentially leading to memory errors or poor performance.\n*   **C) Incorrect.** This uses a list comprehension. While concise, a list comprehension immediately constructs an *entire list* of the results in memory before returning it. This violates the requirement for memory-efficiency when dealing with a \"very large\" stream. A generator expression `(x * x for x in data_stream if x % 3 == 0)` would have been memory-efficient, but this option uses a list comprehension.\n*   **D) Incorrect.** This option uses a global list `_temp_results` to store all the squared numbers. This is both bad practice (due to reliance on global mutable state) and, more importantly, it stores all results in memory, failing the memory-efficiency requirement for a \"very large\" stream. `iter(_temp_results)` only makes the *already created* list iterable, it doesn't make the process memory-efficient."
}

{
  "timestamp": "2026-02-21 09:28:51 UTC",
  "response": "Here are three multiple-choice questions (MCQs) designed to meet your specifications:\n\n---\n\n### Question 1: AWS Architect (Intermediate Level)\n\n**The Question:**\n\nA company hosts a dynamic web application on EC2 instances behind an Application Load Balancer (ALB), with data stored in Amazon RDS. They observe high latency for users accessing static assets (images, CSS, JavaScript) and want to reduce load on their EC2 instances while improving global user experience and reducing operational costs. What is the most effective and AWS-native architectural improvement for their static content delivery?\n\n**Answer Options:**\n\nA. Place an Amazon S3 bucket in front of the ALB to cache static assets.\nB. Configure Amazon CloudFront with an Amazon S3 bucket as its origin for static assets.\nC. Implement auto-scaling for EC2 instances based on CPU utilization and serve static assets directly from EC2.\nD. Enable cross-Region replication for the RDS database and place EC2 instances in multiple Regions.\n\n**Correct Answer:** B\n\n**Explanation:**\n\n*   **A. Place an Amazon S3 bucket in front of the ALB to cache static assets.** This is incorrect. An S3 bucket is a storage service, not a caching layer meant to sit *in front* of an ALB for a dynamic application. While S3 can store static assets, it wouldn't cache them at edge locations for global delivery, nor would it effectively sit \"in front\" of an ALB.\n*   **B. Configure Amazon CloudFront with an Amazon S3 bucket as its origin for static assets.** This is the correct and most effective solution. Amazon CloudFront is a Content Delivery Network (CDN) service that caches content at edge locations globally, drastically reducing latency for users worldwide. Using an Amazon S3 bucket as the origin for CloudFront is the standard best practice for highly available, scalable, and cost-effective static content delivery, offloading traffic from EC2 instances.\n*   **C. Implement auto-scaling for EC2 instances based on CPU utilization and serve static assets directly from EC2.** This is incorrect. While auto-scaling improves the resilience and scalability of the EC2 fleet for dynamic content, it doesn't address the fundamental inefficiency and latency of serving static assets directly from EC2, especially for geographically dispersed users. It would also increase EC2 costs unnecessarily for static content.\n*   **D. Enable cross-Region replication for the RDS database and place EC2 instances in multiple Regions.** This is incorrect. This strategy improves database resilience and application availability across multiple geographic regions, but it does not specifically optimize or offload the delivery of static assets.\n\n---\n\n### Question 2: Terraform Script (Intermediate Level)\n\n**The Question:**\n\nA team wants to deploy multiple AWS S3 buckets, each with a unique name and specific tag, using a single Terraform configuration. They've defined a variable `bucket_configs` as a map where keys are bucket names and values are objects containing tags.\n\n```terraform\nvariable \"bucket_configs\" {\n  description = \"Map of S3 bucket configurations.\"\n  type = map(object({\n    tags = map(string)\n  }))\n  default = {\n    \"app-logs-prod\" = {\n      tags = {\n        Environment = \"Production\",\n        Project     = \"App\"\n      }\n    },\n    \"data-archive-dev\" = {\n      tags = {\n        Environment = \"Development\",\n        Project     = \"Data\"\n      }\n    }\n  }\n}\n\nresource \"aws_s3_bucket\" \"example\" {\n  for_each = var.bucket_configs\n  bucket   = each.key\n  tags     = each.value.tags\n  # ... other bucket configurations ...\n}\n```\n\nWhich of the following Terraform `output` blocks would correctly export a map of the ARNs for all created S3 buckets, using their bucket names (which are also the `for_each` keys) as the keys in the output map?\n\n**Answer Options:**\n\nA.\n```terraform\noutput \"bucket_arns\" {\n  description = \"ARNs of the created S3 buckets.\"\n  value       = aws_s3_bucket.example[*].arn\n}\n```\nB.\n```terraform\noutput \"bucket_arns\" {\n  description = \"ARNs of the created S3 buckets.\"\n  value       = { for name, bucket in aws_s3_bucket.example : name => bucket.arn }\n}\n```\nC.\n```terraform\noutput \"bucket_arns\" {\n  description = \"ARNs of the created S3 buckets.\"\n  value       = map(aws_s3_bucket.example.*.arn)\n}\n```\nD.\n```terraform\noutput \"bucket_arns\" {\n  description = \"ARNs of the created S3 buckets.\"\n  value       = {\n    for key, val in var.bucket_configs :\n    key => aws_s3_bucket.example[val].arn\n  }\n}\n```\n\n**Correct Answer:** B\n\n**Explanation:**\n\n*   **A. `value = aws_s3_bucket.example[*].arn`** This is incorrect. The splat operator `[*]` is typically used with lists or sets of resources to extract an attribute from each item. Since `aws_s3_bucket.example` is a map (due to `for_each`), using `[*]` here would result in an error or an unexpected list of ARNs without the desired map keys.\n*   **B. `value = { for name, bucket in aws_s3_bucket.example : name => bucket.arn }`** This is the correct solution. It uses a `for` expression to iterate over the `aws_s3_bucket.example` map. `name` captures the key of each resource instance (which corresponds to the bucket name provided by `each.key`), and `bucket` captures the resource object itself. The expression then correctly constructs a new map where the keys are the bucket names (`name`) and the values are their ARNs (`bucket.arn`).\n*   **C. `value = map(aws_s3_bucket.example.*.arn)`** This is incorrect. `map(...)` is not a valid function for converting a list (which `aws_s3_bucket.example.*.arn` might attempt to produce, though incorrectly for a map) into a map with specific keys.\n*   **D. `value = { for key, val in var.bucket_configs : key => aws_s3_bucket.example[val].arn }`** This is incorrect. While it correctly iterates over `var.bucket_configs` to get the keys (`key`), the indexing `aws_s3_bucket.example[val]` is wrong. `val` in this context is the `object` containing tags, not a string key that can be used to reference a specific bucket instance in the `aws_s3_bucket.example` map. It should be `aws_s3_bucket.example[key].arn`.\n\n---\n\n### Question 3: Python Programming (Advanced Level)\n\n**The Question:**\n\nConsider the following Python code snippet that defines a generator function and attempts to process its output in batches:\n\n```python\ndef produce_items():\n    for i in range(1000000):\n        yield i * 2\n\ndef process_batches(item_generator, batch_size):\n    batch = []\n    for item in item_generator:\n        batch.append(item)\n        if len(batch) == batch_size:\n            yield batch\n            batch = []\n    if batch: # Yield any remaining items as a final batch\n        yield batch\n\n# Usage\ngen = produce_items()\nbatch_processor = process_batches(gen, 10000)\n\n# What happens when we iterate through 'batch_processor'?\nfirst_batch = next(batch_processor)\nsecond_batch = next(batch_processor)\n```\n\nWhich of the following statements accurately describes the behavior and characteristics of `process_batches` and its interaction with `produce_items`?\n\n**Answer Options:**\n\nA. The `process_batches` function will load all 1,000,000 items from `produce_items` into memory before yielding the first batch, leading to high memory consumption.\nB. `process_batches` is an efficient generator that consumes items from `item_generator` incrementally, forming batches and yielding them without ever holding more than `batch_size` items in memory at any given time (plus internal generator state).\nC. The `produce_items` generator will be exhausted after `first_batch` is created, and `second_batch` will always be empty because `gen` cannot be resumed.\nD. This code will raise a `StopIteration` error immediately after `first_batch = next(batch_processor)` because a generator cannot take another generator as an argument.\n\n**Correct Answer:** B\n\n**Explanation:**\n\n*   **A. The `process_batches` function will load all 1,000,000 items from `produce_items` into memory before yielding the first batch, leading to high memory consumption.** This is incorrect. `process_batches` is itself a generator (due to the `yield` keyword). It iterates over `item_generator` (which is `produce_items()`) using a `for` loop. This means `process_batches` pulls items from `produce_items` one by one, not all at once. The `batch` list in `process_batches` will at most hold `batch_size` items before being yielded and reset. This design avoids loading all items into memory.\n*   **B. `process_batches` is an efficient generator that consumes items from `item_generator` incrementally, forming batches and yielding them without ever holding more than `batch_size` items in memory at any given time (plus internal generator state).** This is the correct description. Both `produce_items` and `process_batches` are generators, which means they perform lazy evaluation. `process_batches` only requests the next item from `item_generator` when it needs it to form a batch. Once a batch is full, it's yielded, and the internal `batch` list is cleared, ensuring memory usage stays low and proportional to `batch_size`, not the total number of items.\n*   **C. The `produce_items` generator will be exhausted after `first_batch` is created, and `second_batch` will always be empty because `gen` cannot be resumed.** This is incorrect. Generators maintain their state between `yield` calls. When `process_batches` yields `first_batch`, `produce_items` is paused at the point it last yielded an item. When `next(batch_processor)` is called again for `second_batch`, `process_batches` resumes from where it left off, continuing to pull items from the *still active* `produce_items` generator.\n*   **D. This code will raise a `StopIteration` error immediately after `first_batch = next(batch_processor)` because a generator cannot take another generator as an argument.** This is incorrect. Python's iteration protocol allows generators (and any iterable) to be consumed by other functions, including other generators, seamlessly. This is a common and powerful pattern for composing data pipelines in Python."
}

{
  "timestamp": "2025-11-25 09:23:13 UTC",
  "response": "Here are three multiple-choice questions, each adhering to your specified requirements:\n\n---\n\n### **1. AWS Architect (Intermediate Level)**\n\n**Question:** A rapidly growing e-commerce company wants to re-architect its relational product catalog database. The current database runs on a single EC2 instance, leading to performance bottlenecks during peak sales and a single point of failure. The new architecture must support millions of read operations per second, low latency for reads globally (especially for users in different continents), and high availability. Cost-effectiveness is also a key consideration.\n\nWhich AWS database architecture would best meet these requirements?\n\n**A:** Migrate the database to Amazon RDS for MySQL Multi-AZ, provisioned with max IOPS.\n**B:** Implement Amazon Aurora Global Database with multiple Aurora Replicas in each region.\n**C:** Use Amazon DynamoDB in On-Demand capacity mode with Global Tables.\n**D:** Set up a self-managed MySQL cluster across multiple EC2 instances in different regions, using custom replication.\n\n**Correct Answer: B**\n\n**Explanation:**\n*   **B is correct:** Amazon Aurora Global Database is specifically designed for globally distributed applications that require high availability, fast local reads with low latency, and disaster recovery across regions. It allows for a primary region and up to five secondary regions, each capable of hosting multiple Aurora Replicas. These replicas provide massive read scaling (supporting millions of reads per second) by offloading read traffic from the primary instance, ensuring low latency for users worldwide. Aurora's architecture is also highly available and cost-effective compared to self-managed solutions for high-scale relational workloads.\n*   **A is incorrect:** While Amazon RDS for MySQL Multi-AZ provides high availability within a single region and can handle high IOPS, it does not inherently offer low-latency global reads or the immense read scaling capabilities (like Aurora Replicas across regions) required for \"millions of read operations per second\" globally without significant additional architectural complexity. Read replicas for standard RDS are primarily for read scaling within a region or for disaster recovery across regions, not for serving global low-latency reads in the same integrated manner as Aurora Global Database.\n*   **C is incorrect:** Amazon DynamoDB with Global Tables offers excellent performance, scalability, and global distribution for NoSQL workloads. However, the problem explicitly states a \"relational product catalog database.\" Migrating a relational database to a NoSQL database like DynamoDB would necessitate a complete re-architecture of the application's data model and query logic, which is a much larger and different undertaking than selecting an appropriate relational database architecture. It may also not be suitable if complex relational queries (e.g., joins) are critical.\n*   **D is incorrect:** Setting up a self-managed MySQL cluster across multiple EC2 instances in different regions is technically possible but goes against AWS best practices for critical, high-scale workloads. It introduces significant operational overhead, complexity in terms of replication management, failover, patching, scaling, and monitoring, making it far less cost-effective and reliable than a fully managed solution like Aurora Global Database.\n\n---\n\n### **2. Terraform Script (Intermediate Level)**\n\n**Scenario:** You need to create several AWS S3 buckets, each named according to a specific pattern, and then output the full ARN of each created bucket. The bucket names are provided in a local map variable.\n\n**`local.tf`:**\n```terraform\nlocals {\n  bucket_configs = {\n    app_data = {\n      name = \"my-company-app-data-bucket\"\n      tags = { Environment = \"Production\", Purpose = \"Application Data\" }\n    },\n    logs = {\n      name = \"my-company-logs-bucket\"\n      tags = { Environment = \"Production\", Purpose = \"Logs\" }\n    }\n  }\n}\n```\n\n**`main.tf` (partial):**\n```terraform\nresource \"aws_s3_bucket\" \"example\" {\n  for_each = local.bucket_configs\n  bucket   = each.value.name\n  tags     = each.value.tags\n}\n\n# You need to add an output block here to get a map of all bucket ARNs,\n# where the key is the original map key (e.g., \"app_data\", \"logs\").\n```\n\n**Question:** Which `output` block would correctly produce a map where keys are `app_data` and `logs`, and values are the ARN of the corresponding S3 bucket?\n\n**A:**\n```terraform\noutput \"bucket_arns\" {\n  value = aws_s3_bucket.example.*.arn\n}\n```\n**B:**\n```terraform\noutput \"bucket_arns\" {\n  value = [for key, bucket in aws_s3_bucket.example : bucket.arn]\n}\n```\n**C:**\n```terraform\noutput \"bucket_arns\" {\n  value = {\n    for key, config in local.bucket_configs :\n    key => aws_s3_bucket.example[key].arn\n  }\n}\n```\n**D:**\n```terraform\noutput \"bucket_arns\" {\n  value = aws_s3_bucket.example[each.key].arn\n}\n```\n\n**Correct Answer: C**\n\n**Explanation:**\n*   **C is correct:** This `output` block uses a `for` expression to iterate over the `local.bucket_configs` map. For each `key` (e.g., \"app_data\", \"logs\") and `config` (the inner map `{ name=..., tags=... }`) in `local.bucket_configs`, it then accesses the corresponding S3 bucket resource created by `for_each` using `aws_s3_bucket.example[key]`. The `arn` attribute of this resource is then assigned as the value to that `key` in the output map. This precisely matches the requirement of producing a map with the original map keys and the ARNs of the created buckets.\n*   **A is incorrect:** The `*.arn` (splat expression) syntax is used to extract an attribute from a *list* of resources. When `for_each` is used, `aws_s3_bucket.example` becomes a *map* of resource instances. Applying `*.arn` to a map of resources will result in an error or an unexpected output type (e.g., a list of all ARNs if Terraform can coerce it, but not a map with the desired keys).\n*   **B is incorrect:** This `for` expression uses the list comprehension syntax (`[ ... ]`) which will produce a *list* of ARNs, not a *map* with the original keys. The requirement explicitly asks for a map where the keys are \"app_data\" and \"logs\".\n*   **D is incorrect:** The `each.key` syntax is only valid within a `for_each` block (or `for_each` on a module), where `each` refers to the current iteration element. It cannot be used standalone in an `output` block to implicitly iterate over a collection of resources.\n\n---\n\n### **3. Python Programming (Advanced Level)**\n\n**Scenario:** You are developing a system where certain resource-intensive functions should only be called a limited number of times during the program's execution to prevent abuse or control usage. You need a decorator that enforces this limit.\n\n**Question:** Design a Python decorator named `limited_calls` that takes an integer `limit` as an argument. The decorated function should raise a `RuntimeError` if it is called more times than `limit`. The decorator should *not* rely on external libraries (standard library components like `functools` are permitted).\n\nWhich of the following implementations correctly implements the `limited_calls` decorator?\n\n**A:**\n```python\ndef limited_calls(limit):\n    call_count = [0] # Initialize count in outer scope as a mutable list\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            if call_count[0] >= limit:\n                raise RuntimeError(f\"Function '{func.__name__}' called too many times.\")\n            call_count[0] += 1\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n```\n**B:**\n```python\nimport functools\n\ndef limited_calls(limit):\n    def decorator(func):\n        calls_made = [0] # Correctly placed *inside* decorator for per-function state\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            if calls_made[0] >= limit:\n                raise RuntimeError(f\"Function '{func.__name__}' called too many times.\")\n            calls_made[0] += 1\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n```\n**C:**\n```python\nimport functools\n\ndef limited_calls(limit):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            wrapper.call_count = 0 # ERROR: Resets count on every call\n            if wrapper.call_count >= limit:\n                raise RuntimeError(f\"Function '{func.__name__}' called too many times.\")\n            wrapper.call_count += 1\n            return func(*args, **kwargs)\n        return decorator\n    return decorator\n```\n**D:**\n```python\nimport functools\n\ndef limited_calls(limit):\n    call_count = 0\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            if call_count >= limit:\n                raise RuntimeError(f\"Function '{func.__name__}' called too many times.\")\n            call_count += 1 # This will cause an UnboundLocalError\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n```\n\n**Correct Answer: B**\n\n**Explanation:**\n*   **B is correct:** This implementation correctly uses a closure to maintain independent state for each decorated function.\n    *   `calls_made = [0]` is initialized *inside* the `decorator` function. This is crucial because it ensures that *each time* `limited_calls(limit)` is used to decorate a *new* function, a fresh `calls_made` list is created and captured by the `wrapper`'s closure. This makes the state unique to that specific decorated function instance.\n    *   Using a mutable list (`[0]`) allows the `wrapper` function to modify `calls_made[0]` (incrementing the count) without needing the `nonlocal` keyword, as it's modifying an element of a captured mutable object, not reassigning the `calls_made` variable itself.\n    *   `@functools.wraps(func)` is correctly used as a best practice to preserve the original function's metadata (like `__name__`, `__doc__`, etc.), which is important for debugging and introspection.\n*   **A is incorrect:** In this option, `call_count = [0]` is initialized in the outer `limited_calls` function's scope. If `limited_calls` were called once and its returned `decorator` function was then applied to *multiple different functions*, all those decorated functions would *share the same `call_count` list*, leading to a combined limit rather than individual limits. While `limited_calls(N)` is typically called per function (e.g., `@limited_calls(2)\\ndef foo(): ...`), the `functools.wraps` best practice is also omitted.\n*   **C is incorrect:** The line `wrapper.call_count = 0` inside the `wrapper` function will *reset the call count to 0 on every single invocation* of the decorated function. This completely defeats the purpose of the decorator, as the function would effectively always be callable (unless `limit` is 0 or 1 on the very first call). To correctly store state as an attribute, it should be initialized only once, for example, `if not hasattr(wrapper, 'call_count'): wrapper.call_count = 0`.\n*   **D is incorrect:** The variable `call_count` is initialized as an integer (`call_count = 0`) in the `limited_calls` scope. Inside the `wrapper` function, the line `call_count += 1` attempts to modify `call_count`. Since `call_count` is assigned to (`+=` implies assignment), Python treats `call_count` as a new *local* variable within `wrapper`. However, it's not initialized within `wrapper` before being incremented, leading to an `UnboundLocalError`. To modify a non-local variable from an enclosing scope, `nonlocal call_count` would be required, but then `call_count` would need to be defined in `decorator`'s scope (not `limited_calls`'s scope) for per-function state."
}
